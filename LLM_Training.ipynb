
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-cri-spy/LLM-Playground/blob/main/LLM_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtfswpA_nxkA"
      },
      "source": [
        "Set up the development environment to utilize GPU resources.\n",
        "Understand and install specific library versions directly from a repository.\n",
        "Familiarize with YAML configuration for training setups.\n",
        "Execute a basic training session for a language model using the Axolotl library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB5BGMi0n3vg",
        "outputId": "a917f281-d76e-42fd-ec41-86270c200aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.12/dist-packages (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XxdaNds3n4XJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\n",
        "assert (torch.cuda.is_available()==True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6lbx6kvok1X"
      },
      "source": [
        "Install the Axolotl library directly from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLepVt2SozGd",
        "outputId": "bf2f6774-e91d-48ed-afd4-3ddc6022bf99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/hqq_aten-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining axolotl from git+https://github.com/axolotl-ai-cloud/axolotl.git@78b42a3fe13c49e317bc116b9999c30e070322cc#egg=axolotl\n",
            "  Skipping because already up-to-date.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers.git@026a173a64372e9602a16523b8fae9de4b0ff428 (from axolotl)\n",
            "  Cloning https://github.com/huggingface/transformers.git (to revision 026a173a64372e9602a16523b8fae9de4b0ff428) to /tmp/pip-install-7533_1i0/transformers_43cfb910f93642eb969844b078c60fde\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-7533_1i0/transformers_43cfb910f93642eb969844b078c60fde\n",
            "  Running command git rev-parse -q --verify 'sha^026a173a64372e9602a16523b8fae9de4b0ff428'\n",
            "  Running command git fetch -q https://github.com/huggingface/transformers.git 026a173a64372e9602a16523b8fae9de4b0ff428\n",
            "  Running command git checkout -q 026a173a64372e9602a16523b8fae9de4b0ff428\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 026a173a64372e9602a16523b8fae9de4b0ff428\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe (from axolotl)\n",
            "  Cloning https://github.com/lm-sys/FastChat.git (to revision 27a05b04a35510afb1d767ae7e5990cbd278f8fe) to /tmp/pip-install-7533_1i0/fschat_16cca09f7084497aa538726a0956a864\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lm-sys/FastChat.git /tmp/pip-install-7533_1i0/fschat_16cca09f7084497aa538726a0956a864\n",
            "  Running command git rev-parse -q --verify 'sha^27a05b04a35510afb1d767ae7e5990cbd278f8fe'\n",
            "  Running command git fetch -q https://github.com/lm-sys/FastChat.git 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n",
            "  Running command git checkout -q 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n",
            "  Resolved https://github.com/lm-sys/FastChat.git to commit 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging==23.2 in /usr/local/lib/python3.12/dist-packages (from axolotl) (23.2)\n",
            "Collecting peft==0.11.1 (from axolotl)\n",
            "  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tokenizers==0.19.1 (from axolotl)\n",
            "  Using cached tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting bitsandbytes==0.43.3 (from axolotl)\n",
            "  Using cached bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting accelerate==0.32.0 (from axolotl)\n",
            "  Using cached accelerate-0.32.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pydantic==2.6.3 (from axolotl)\n",
            "  Using cached pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (from axolotl) (2.4.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.7.1)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from axolotl) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from axolotl) (2.32.3)\n",
            "Collecting datasets==2.19.1 (from axolotl)\n",
            "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.2.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.21.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.8.1)\n",
            "Requirement already satisfied: optimum==1.16.2 in /usr/local/lib/python3.12/dist-packages (from axolotl) (1.16.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.1.9)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.4.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from axolotl) (1.26.4)\n",
            "Requirement already satisfied: evaluate==0.4.1 in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from axolotl) (1.16.1)\n",
            "Collecting scikit-learn==1.2.2 (from axolotl)\n",
            "  Using cached scikit-learn-1.2.2.tar.gz (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (from axolotl) (12.0.0)\n",
            "Requirement already satisfied: art in /usr/local/lib/python3.12/dist-packages (from axolotl) (6.5)\n",
            "Requirement already satisfied: gradio==3.50.2 in /usr/local/lib/python3.12/dist-packages (from axolotl) (3.50.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from axolotl) (2.19.0)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/lib/python3.12/dist-packages (from axolotl) (1.0.1)\n",
            "Requirement already satisfied: autoawq>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.2.7.post3)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/dist-packages (from axolotl) (2024.9.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.12/dist-packages (from axolotl) (2024.9.0.post1)\n",
            "Collecting trl==0.9.6 (from axolotl)\n",
            "  Using cached trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: zstandard==0.22.0 in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.22.0)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.12/dist-packages (from axolotl) (1.8.7)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.12/dist-packages (from axolotl) (2.4.0)\n",
            "Requirement already satisfied: xformers>=0.0.26.post1 in /usr/local/lib/python3.12/dist-packages (from axolotl) (0.0.27.post2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.32.0->axolotl) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.32.0->axolotl) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.32.0->axolotl) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.1->axolotl)\n",
            "  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (0.70.16)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1->axolotl)\n",
            "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->axolotl) (3.12.15)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.1->axolotl) (0.18.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==0.6.1 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (0.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (3.11.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (10.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (0.0.20)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (0.35.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.50.2->axolotl) (11.0.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from optimum==1.16.2->axolotl) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from optimum==1.16.2->axolotl) (1.13.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.6.3->axolotl) (0.7.0)\n",
            "Collecting pydantic-core==2.16.3 (from pydantic==2.6.3->axolotl)\n",
            "  Using cached pydantic_core-2.16.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.2.2->axolotl) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.2.2->axolotl) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (75.8.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->axolotl) (3.0.0)\n",
            "Collecting tyro>=0.5.11 (from trl==0.9.6->axolotl)\n",
            "  Using cached tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->axolotl) (12.6.85)\n",
            "INFO: pip is looking at multiple versions of autoawq to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting autoawq>=0.2.5 (from axolotl)\n",
            "  Using cached autoawq-0.2.9.tar.gz (74 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached autoawq-0.2.8.tar.gz (71 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached autoawq-0.2.7.post2-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached autoawq-0.2.7.post1-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached autoawq-0.2.7-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting optimum==1.16.2 (from axolotl)\n",
            "  Using cached optimum-1.16.2-py3-none-any.whl.metadata (17 kB)\n",
            "INFO: pip is still looking at multiple versions of autoawq to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting evaluate==0.4.1 (from axolotl)\n",
            "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "\u001b[31mERROR: Cannot install autoawq==0.2.7.post3, axolotl and axolotl==0.4.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    axolotl 0.4.1 depends on transformers 4.44.0.dev0 (from git+https://github.com/huggingface/transformers.git@026a173a64372e9602a16523b8fae9de4b0ff428)\n",
            "    optimum 1.16.2 depends on transformers>=4.26.0\n",
            "    peft 0.11.1 depends on transformers\n",
            "    trl 0.9.6 depends on transformers>=4.31.0\n",
            "    autoawq 0.2.7.post3 depends on transformers>=4.45.0\n",
            "    axolotl 0.4.1 depends on transformers 4.44.0.dev0 (from git+https://github.com/huggingface/transformers.git@026a173a64372e9602a16523b8fae9de4b0ff428)\n",
            "    optimum 1.16.2 depends on transformers>=4.26.0\n",
            "    peft 0.11.1 depends on transformers\n",
            "    trl 0.9.6 depends on transformers>=4.31.0\n",
            "    autoawq 0.2.9 depends on transformers>=4.45.0\n",
            "    axolotl 0.4.1 depends on transformers 4.44.0.dev0 (from git+https://github.com/huggingface/transformers.git@026a173a64372e9602a16523b8fae9de4b0ff428)\n",
            "    optimum 1.16.2 depends on transformers>=4.26.0\n",
            "    peft 0.11.1 depends on transformers\n",
            "    trl 0.9.6 depends on transformers>=4.31.0\n",
            "    autoawq 0.2.8 depends on transformers<=4.47.1 and >=4.45.0\n",
            "    axolotl 0.4.1 depends on datasets==2.19.1\n",
            "    evaluate 0.4.1 depends on datasets>=2.0.0\n",
            "    optimum 1.16.2 depends on datasets\n",
            "    trl 0.9.6 depends on datasets\n",
            "    autoawq 0.2.7.post2 depends on datasets>=2.20\n",
            "    axolotl 0.4.1 depends on datasets==2.19.1\n",
            "    evaluate 0.4.1 depends on datasets>=2.0.0\n",
            "    optimum 1.16.2 depends on datasets\n",
            "    trl 0.9.6 depends on datasets\n",
            "    autoawq 0.2.7.post1 depends on datasets>=2.20\n",
            "    axolotl 0.4.1 depends on torch==2.4.0\n",
            "    accelerate 0.32.0 depends on torch>=1.10.0\n",
            "    bitsandbytes 0.43.3 depends on torch\n",
            "    optimum 1.16.2 depends on torch>=1.11\n",
            "    peft 0.11.1 depends on torch>=1.13.0\n",
            "    trl 0.9.6 depends on torch>=1.4.0\n",
            "    autoawq 0.2.7 depends on torch>=2.5.1\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -e 'git+https://github.com/axolotl-ai-cloud/axolotl.git@78b42a3fe13c49e317bc116b9999c30e070322cc#egg=axolotl' # ensures the same version we used in the course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFNr7vKBpsdb"
      },
      "source": [
        "#Configuration Setup\n",
        "Create a YAML configuration to meticulously set up the training parameters. This configuration file will include settings for the model, tokenizer, and training details, structured to work efficiently even on less powerful, free tier GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0uRef-KpV1y"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "train_config = \"\"\"\n",
        "# model params\n",
        "base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: LlamaTokenizer\n",
        "\n",
        "\n",
        "# dataset params\n",
        "datasets:\n",
        "  - path: jaydenccc/AI_Storyteller_Dataset\n",
        "    type:\n",
        "      system_prompt: \"\"\n",
        "      field_system: system\n",
        "      field_instruction: synopsis\n",
        "      field_output: short_story\n",
        "      format: \"<|user|>\\n {instruction} </s>\\n<|assistant|>\"\n",
        "      no_input_format: \"<|user|> {instruction} </s>\\n<|assistant|>\"\n",
        "\n",
        "output_dir: ./models/TinyLlama_Storyteller\n",
        "\n",
        "# model params\n",
        "sequence_length: 1024\n",
        "bf16: auto\n",
        "tf32: false\n",
        "\n",
        "# training params\n",
        "batch_size: 4\n",
        "micro_batch_size: 4\n",
        "num_epochs: 2\n",
        "optimizer: adamw_bnb_8bit\n",
        "learning_rate: 0.0002\n",
        "\n",
        "logging_steps: 1\n",
        "\"\"\"\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(train_config)\n",
        "\n",
        "\n",
        "# Write the YAML file\n",
        "with open(\"basic_train.yml\", 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mOTQMwQxirT"
      },
      "source": [
        "Launch the training process with the accelerate command. This command is optimized for use even with free-tier resources, ensuring that you can train models effectively without requiring premium hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y49iZfMHxkTk",
        "outputId": "0fee1e3a-9a33-4a6d-e057-936dbb02ae3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-08-19 19:48:55.997141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755632936.287969   19752 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755632936.365267   19752 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755632936.948279   19752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755632936.948317   19752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755632936.948322   19752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755632936.948330   19752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-19 19:48:57.006722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-08-19 19:49:03,856] [INFO] [datasets.<module>:54] [PID:19752] PyTorch version 2.4.0 available.\n",
            "[2025-08-19 19:49:03,857] [INFO] [datasets.<module>:66] [PID:19752] Polars version 1.25.2 available.\n",
            "[2025-08-19 19:49:03,858] [INFO] [datasets.<module>:77] [PID:19752] Duckdb version 1.3.2 available.\n",
            "[2025-08-19 19:49:03,859] [INFO] [datasets.<module>:112] [PID:19752] TensorFlow version 2.19.0 available.\n",
            "[2025-08-19 19:49:03,860] [INFO] [datasets.<module>:125] [PID:19752] JAX version 0.5.3 available.\n",
            "[2025-08-19 19:49:06,329] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-08-19 19:49:06,402] [INFO] [root.spawn:60] [PID:19752] x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmp90e5oqah/test.c -o /tmp/tmp90e5oqah/test.o\n",
            "[2025-08-19 19:49:06,446] [INFO] [root.spawn:60] [PID:19752] x86_64-linux-gnu-gcc /tmp/tmp90e5oqah/test.o -laio -o /tmp/tmp90e5oqah/a.out\n",
            "[2025-08-19 19:49:07,115] [INFO] [root.spawn:60] [PID:19752] x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmp039wbjou/test.c -o /tmp/tmp039wbjou/test.o\n",
            "[2025-08-19 19:49:07,132] [INFO] [root.spawn:60] [PID:19752] x86_64-linux-gnu-gcc /tmp/tmp039wbjou/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmp039wbjou/a.out\n",
            "[2025-08-19 19:49:07,182] [INFO] [root.spawn:60] [PID:19752] x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmp7k568xh7/test.c -o /tmp/tmp7k568xh7/test.o\n",
            "[2025-08-19 19:49:07,198] [INFO] [root.spawn:60] [PID:19752] x86_64-linux-gnu-gcc /tmp/tmp7k568xh7/test.o -laio -o /tmp/tmp7k568xh7/a.out\n",
            "/usr/local/lib/python3.12/dist-packages/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
            "\u001b[33m[2025-08-19 19:49:10,955] [WARNING] [axolotl.utils.config.models.input.hint_batch_size_set:538] [PID:19752] [RANK:0] batch_size is not recommended. Please use gradient_accumulation_steps instead.\n",
            "To calculate the equivalent gradient_accumulation_steps, divide batch_size / micro_batch_size / number of gpus.\u001b[39m\n",
            "[2025-08-19 19:49:10,994] [DEBUG] [axolotl.resolve_dtype:67] [PID:19752] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "config.json: 100% 608/608 [00:00<00:00, 4.51MB/s]\n",
            "[2025-08-19 19:49:11,181] [INFO] [axolotl.normalize_config:236] [PID:19752] [RANK:0] cuda memory usage baseline: 0.000GB (+0.002GB cache, +0.359GB misc)\u001b[39m\n",
            "\n",
            "     #@@ #@@      @@# @@#\n",
            "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
            "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
            "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
            "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
            "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
            "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
            "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
            "    @@@@  @@@@@@@@@@@@@@@@\n",
            "\n",
            "\u001b[33m[2025-08-19 19:49:11,200] [WARNING] [axolotl.cli.checks.check_user_token:47] [PID:19752] [RANK:0] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.\u001b[39m\n",
            "tokenizer_config.json: 1.29kB [00:00, 6.58MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 1.05MB/s]\n",
            "special_tokens_map.json: 100% 551/551 [00:00<00:00, 5.16MB/s]\n",
            "tokenizer.json: 1.84MB [00:00, 54.9MB/s]\n",
            "[2025-08-19 19:49:12,648] [DEBUG] [axolotl.load_tokenizer:298] [PID:19752] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2025-08-19 19:49:12,648] [DEBUG] [axolotl.load_tokenizer:299] [PID:19752] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2025-08-19 19:49:12,648] [DEBUG] [axolotl.load_tokenizer:300] [PID:19752] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2025-08-19 19:49:12,648] [DEBUG] [axolotl.load_tokenizer:301] [PID:19752] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2025-08-19 19:49:12,648] [INFO] [axolotl.load_tokenizer:315] [PID:19752] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2025-08-19 19:49:12,649] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:252] [PID:19752] [RANK:0] Unable to find prepared dataset in last_run_prepared/4973c5008e30e013c2da0e704ead90ec\u001b[39m\n",
            "[2025-08-19 19:49:12,649] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:253] [PID:19752] [RANK:0] Loading raw datasets...\u001b[39m\n",
            "\u001b[33m[2025-08-19 19:49:12,649] [WARNING] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:255] [PID:19752] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
            "[2025-08-19 19:49:12,649] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:262] [PID:19752] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
            "README.md: 100% 404/404 [00:00<00:00, 3.14MB/s]\n",
            "data/train-00000-of-00001-532ad934f217d0(…): 100% 130k/130k [00:00<00:00, 135kB/s]\n",
            "Generating train split: 100% 100/100 [00:00<00:00, 1042.52 examples/s]\n",
            "[2025-08-19 19:49:21,334] [INFO] [axolotl.utils.data.sft.get_dataset_wrapper:457] [PID:19752] [RANK:0] Loading dataset with base_type: None and prompt_style: None\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2): 100% 100/100 [00:00<00:00, 215.74 examples/s]\n",
            "[2025-08-19 19:49:21,902] [DEBUG] [axolotl.utils.data.utils.drop_long_seq_in_dataset:176] [PID:19752] [RANK:0] min_input_len: 338\u001b[39m\n",
            "[2025-08-19 19:49:21,902] [DEBUG] [axolotl.utils.data.utils.drop_long_seq_in_dataset:178] [PID:19752] [RANK:0] max_input_len: 547\u001b[39m\n",
            "Dropping Long Sequences (num_proc=2): 100% 100/100 [00:00<00:00, 596.90 examples/s]\n",
            "\u001b[33m[2025-08-19 19:49:22,160] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:206] [PID:19752] [RANK:0] Dropped 42 long samples from dataset\u001b[39m\n",
            "[2025-08-19 19:49:22,160] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:332] [PID:19752] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/4973c5008e30e013c2da0e704ead90ec\u001b[39m\n",
            "Saving the dataset (1/1 shards): 100% 58/58 [00:00<00:00, 11300.68 examples/s]\n",
            "[2025-08-19 19:49:22,201] [DEBUG] [axolotl.calculate_total_num_steps:403] [PID:19752] [RANK:0] total_num_tokens: 25_722\u001b[39m\n",
            "[2025-08-19 19:49:22,203] [DEBUG] [axolotl.calculate_total_num_steps:421] [PID:19752] [RANK:0] `total_supervised_tokens: 45_123`\u001b[39m\n",
            "[2025-08-19 19:49:22,203] [DEBUG] [axolotl.calculate_total_num_steps:499] [PID:19752] [RANK:0] total_num_steps: 29\u001b[39m\n",
            "[2025-08-19 19:49:22,208] [DEBUG] [axolotl.train.train:47] [PID:19752] [RANK:0] loading tokenizer... TinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\n",
            "[2025-08-19 19:49:22,490] [DEBUG] [axolotl.load_tokenizer:298] [PID:19752] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2025-08-19 19:49:22,490] [DEBUG] [axolotl.load_tokenizer:299] [PID:19752] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2025-08-19 19:49:22,490] [DEBUG] [axolotl.load_tokenizer:300] [PID:19752] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2025-08-19 19:49:22,491] [DEBUG] [axolotl.load_tokenizer:301] [PID:19752] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2025-08-19 19:49:22,491] [INFO] [axolotl.load_tokenizer:315] [PID:19752] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2025-08-19 19:49:22,491] [DEBUG] [axolotl.train.train:82] [PID:19752] [RANK:0] loading model\u001b[39m\n",
            "model.safetensors: 100% 2.20G/2.20G [00:52<00:00, 41.7MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 930kB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/axolotl/core/trainers/base.py:177: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*_args, **kwargs)\n",
            "[2025-08-19 19:50:25,568] [INFO] [axolotl.train.train:173] [PID:19752] [RANK:0] Starting trainer...\u001b[39m\n",
            "{'loss': 1.4479, 'grad_norm': 6.611915588378906, 'learning_rate': 0.00019945218953682734, 'epoch': 0.07}\n",
            "  3% 1/30 [00:06<03:05,  6.39s/it][2025-08-19 19:50:37,749] [INFO] [axolotl.callbacks.on_step_end:127] [PID:19752] [RANK:0] cuda memory usage while training: 4.445GB (+7.619GB cache, +0.392GB misc)\u001b[39m\n",
            "{'loss': 3.9614, 'grad_norm': 39.97809600830078, 'learning_rate': 0.00019781476007338058, 'epoch': 0.13}\n",
            "{'loss': 3.2806, 'grad_norm': 71.659423828125, 'learning_rate': 0.00019510565162951537, 'epoch': 0.2}\n",
            "{'loss': 2.7401, 'grad_norm': 10.531866073608398, 'learning_rate': 0.0001913545457642601, 'epoch': 0.27}\n",
            "{'loss': 2.3671, 'grad_norm': 8.626730918884277, 'learning_rate': 0.00018660254037844388, 'epoch': 0.33}\n",
            "{'loss': 2.8638, 'grad_norm': 42.756370544433594, 'learning_rate': 0.00018090169943749476, 'epoch': 0.4}\n",
            "{'loss': 2.7051, 'grad_norm': 26.041534423828125, 'learning_rate': 0.00017431448254773944, 'epoch': 0.47}\n",
            "{'loss': 2.325, 'grad_norm': 6.268996238708496, 'learning_rate': 0.00016691306063588583, 'epoch': 0.53}\n",
            "{'loss': 2.4161, 'grad_norm': 5.723141670227051, 'learning_rate': 0.00015877852522924732, 'epoch': 0.6}\n",
            "{'loss': 2.2476, 'grad_norm': 6.332674980163574, 'learning_rate': 0.00015000000000000001, 'epoch': 0.67}\n",
            "{'loss': 2.3327, 'grad_norm': 10.894877433776855, 'learning_rate': 0.00014067366430758004, 'epoch': 0.73}\n",
            "{'loss': 2.281, 'grad_norm': 7.571231842041016, 'learning_rate': 0.00013090169943749476, 'epoch': 0.8}\n",
            "{'loss': 2.2687, 'grad_norm': 6.092329978942871, 'learning_rate': 0.00012079116908177593, 'epoch': 0.87}\n",
            "{'loss': 2.3395, 'grad_norm': 9.633159637451172, 'learning_rate': 0.00011045284632676536, 'epoch': 0.93}\n",
            "{'loss': 2.2766, 'grad_norm': 9.125582695007324, 'learning_rate': 0.0001, 'epoch': 1.0}\n",
            "{'loss': 1.1476, 'grad_norm': 5.696441173553467, 'learning_rate': 8.954715367323468e-05, 'epoch': 1.07}\n",
            "{'loss': 0.9998, 'grad_norm': 5.890514373779297, 'learning_rate': 7.920883091822408e-05, 'epoch': 1.13}\n",
            "{'loss': 1.3818, 'grad_norm': 5.176191806793213, 'learning_rate': 6.909830056250527e-05, 'epoch': 1.2}\n",
            "{'loss': 1.145, 'grad_norm': 5.096940994262695, 'learning_rate': 5.9326335692419995e-05, 'epoch': 1.27}\n",
            "{'loss': 1.224, 'grad_norm': 15.404840469360352, 'learning_rate': 5.000000000000002e-05, 'epoch': 1.33}\n",
            "{'loss': 1.0232, 'grad_norm': 5.02459716796875, 'learning_rate': 4.12214747707527e-05, 'epoch': 1.4}\n",
            "{'loss': 1.1849, 'grad_norm': 4.40369987487793, 'learning_rate': 3.308693936411421e-05, 'epoch': 1.47}\n",
            "{'loss': 1.085, 'grad_norm': 4.440859317779541, 'learning_rate': 2.5685517452260567e-05, 'epoch': 1.53}\n",
            "{'loss': 1.0449, 'grad_norm': 4.296169757843018, 'learning_rate': 1.9098300562505266e-05, 'epoch': 1.6}\n",
            "{'loss': 1.0029, 'grad_norm': 3.9763145446777344, 'learning_rate': 1.339745962155613e-05, 'epoch': 1.67}\n",
            "{'loss': 1.0088, 'grad_norm': 4.07460355758667, 'learning_rate': 8.645454235739903e-06, 'epoch': 1.73}\n",
            "{'loss': 0.7699, 'grad_norm': 3.6691198348999023, 'learning_rate': 4.8943483704846475e-06, 'epoch': 1.8}\n",
            "{'loss': 0.9095, 'grad_norm': 3.4948086738586426, 'learning_rate': 2.1852399266194314e-06, 'epoch': 1.87}\n",
            "{'loss': 0.8011, 'grad_norm': 4.707437038421631, 'learning_rate': 5.478104631726711e-07, 'epoch': 1.93}\n",
            "{'loss': 0.8828, 'grad_norm': 6.223214626312256, 'learning_rate': 0.0, 'epoch': 2.0}\n",
            "{'train_runtime': 510.2804, 'train_samples_per_second': 0.227, 'train_steps_per_second': 0.059, 'train_loss': 1.7821516811847686, 'epoch': 2.0}\n",
            "100% 30/30 [08:30<00:00, 17.01s/it]\n",
            "[2025-08-19 19:58:56,438] [INFO] [axolotl.train.train:192] [PID:19752] [RANK:0] Training Completed!!! Saving pre-trained model to ./models/TinyLlama_Storyteller\u001b[39m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!accelerate launch -m axolotl.cli.train basic_train.yml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNfpeaY5WAeOkl09iPGviqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
