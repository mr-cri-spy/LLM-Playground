{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONp6Od+MfeRtOThybYRamv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-cri-spy/LLM-Playground/blob/main/LLM_BASIC_TO_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSm0EWnh1k9y",
        "outputId": "a0c0ff24-3032-4d42-fb02-f53c85bb5df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['machine', 'deep']\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "text = \"i love machine learning and i love deep learning\"\n",
        "words = text.split()\n",
        "\n",
        "bigrams = defaultdict(list)\n",
        "\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams[words[i]].append(words[i+1])\n",
        "\n",
        "def predict_next_word(word):\n",
        "    return bigrams[word]\n",
        "\n",
        "print(predict_next_word(\"love\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "query = np.array([1, 0, 1])\n",
        "keys = np.array([[1, 0, 0],\n",
        "                 [0, 1, 0],\n",
        "                 [1, 1, 0]])\n",
        "\n",
        "values = np.array([[10, 0],\n",
        "                   [0, 10],\n",
        "                   [5, 5]])\n",
        "\n",
        "scores = np.dot(keys, query)\n",
        "weights = np.exp(scores) / np.sum(np.exp(scores))\n",
        "output = np.dot(weights, values)\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1dCreE4HXCy",
        "outputId": "4a7274fb-f98a-4bcb-c2a0-e2fc905000a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.33478197 3.66521803]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "text = \"I love learning transformers\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "prediction = torch.argmax(logits, dim=1)\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWBwTCgaLAqn",
        "outputId": "845086aa-ac90-44fa-c8f3-20fdcb4ecf2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "prompt = \"Explain transformers in simple words\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcjWkQDHL3ft",
        "outputId": "b0377b55-dab4-4748-ed9e-3d5d27050260"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain transformers in simple words.\n",
            "\n",
            "The following code snippet shows how to create a simple transformer for a simple string.\n",
            "\n",
            "import { String } from './transformers'; import { String } from './transformers/string\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "text = \"summarize: Transformers are powerful deep learning models used in NLP.\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=40)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu6vQfGUM4w3",
        "outputId": "f5624cb0-5268-47f0-e049-da7ea0de26d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are powerful deep learning models used in NLP.\n"
          ]
        }
      ]
    }
  ]
}