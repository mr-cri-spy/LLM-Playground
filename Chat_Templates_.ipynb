



{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlrfiykeT8dZtx23bzOcuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-cri-spy/LLM-Playground/blob/main/Chat_Templates_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Objectives\n",
        "\n",
        "Understand how to install specific versions of the transformers library.\n",
        "Learn the basics of using chat templates with language models.\n",
        "Explore the functionality of the AutoTokenizer for managing conversation flow.\n",
        "Introduction\n",
        "In this notebook, we will delve into the use of chat templates which aid in structuring conversations for language models, ensuring more coherent and context-aware responses.\n",
        "\n",
        "Installation of Transformers Library\n",
        "Before we start, we need to ensure that the correct version of the transformers library is installed. This step is crucial as different versions may have different compatibilities with the models and methods we intend to use."
      ],
      "metadata": {
        "id": "PUvPuqjHbj8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.41.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m7qsgHhgi6T",
        "outputId": "b01c6b7d-8156-4f74-9ff8-b08213f2548b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.41.0 in /usr/local/lib/python3.11/dist-packages (4.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required Modules\n",
        "\n",
        "Once the necessary library is installed, we need to import the tokenizer which will help us in processing text for the language model."
      ],
      "metadata": {
        "id": "euEN_E6QoJTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "RV8tEgQfdd_p"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Up Chat Templates\n",
        "\n",
        "Chat templates are essential for structuring the input and output in a conversation with a language model. They help in maintaining context and generating relevant responses."
      ],
      "metadata": {
        "id": "TJ1JEXYNodiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define example messages in a conversation\n",
        "messages = [\n",
        "   {\"role\": \"user\", \"content\": \"How do chat templates work?\"},\n",
        "   {\"role\": \"assistant\", \"content\": \"Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.\"},\n",
        "   {\"role\": \"user\", \"content\": \"How do I use them?\"},\n",
        "]"
      ],
      "metadata": {
        "id": "wzVVJX9ioHly"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing and Applying Chat Templates\n",
        "\n",
        "We will now use a tokenizer from the transformers library to process these messages and apply chat templates to them. This will prepare our data for interaction with a language model."
      ],
      "metadata": {
        "id": "G8WFNRNyoWoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "blenderbot transformer library\n"
      ],
      "metadata": {
        "id": "LSSuEdpauriH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")"
      ],
      "metadata": {
        "id": "UM5Xj1WisRou"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.apply_chat_template(messages, tokenize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "aP3q8hAAs5Js",
        "outputId": "96b33f16-8a1d-4c2b-a500-593981d9ea05"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' How do chat templates work?  Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.   How do I use them?</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mistral transformer library"
      ],
      "metadata": {
        "id": "7KW5Vxzpu1K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.2\")"
      ],
      "metadata": {
        "id": "ZiK3sye2u9Xj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer1.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj_-gkfPtPs0",
        "outputId": "ce2ce93f-819d-4316-ff25-10f9d513b39f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST] How do chat templates work? [/INST] Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.</s> [INST] How do I use them? [/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemma transformer library"
      ],
      "metadata": {
        "id": "C0RZ78n6u-jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = AutoTokenizer.from_pretrained(\"unsloth/gemma-7b-it\")"
      ],
      "metadata": {
        "id": "I7QpYpVTvEJi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer2.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7zJnJ0fvEYX",
        "outputId": "6431265b-506b-4457-8d12-1d26cdf8a4b6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "How do chat templates work?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "How do I use them?<end_of_turn>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama transformer library"
      ],
      "metadata": {
        "id": "bliDRkxvvE18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerLlama = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-Instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1DAoI13vLtv",
        "outputId": "906dc44a-7049-43d2-85b8-829388240dd2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizerLlama.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBRd-SwLvMEI",
        "outputId": "8935ee8c-0833-4b90-aa6a-2bbd170da96a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How do chat templates work?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How do I use them?<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizerLlama.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L566VS2GyJSz",
        "outputId": "6fbfae8b-622b-4293-a527-406c35d4560f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How do chat templates work?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How do I use them?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
